import os
import re
import hashlib
import argparse
import requests
import pandas as pd
import logging
import glob
import difflib

from bs4 import BeautifulSoup
from datetime import datetime
from tg_bot import send_telegram_message, send_telegram_file

from consts import LOGS_FOLDER, DATA_FOLDER, PARSE_URL, LOGS_LEVEL, KEEP_FILES_COUNT, AIRTABLE_FOLDER

def clean_field(value: str) -> str:
    if not isinstance(value, str):
        return value  # skip cleaning non-string values

    # Remove leading/trailing spaces and unwanted punctuation
    return re.sub(r'^[\s,;.\'\"‚Äú‚Äù¬´¬ª]+|[\s,;.\'\"‚Äú‚Äù¬´¬ª]+$', '', value)

def setup_logger() -> str:
    log_dir = LOGS_FOLDER
    os.makedirs(log_dir, exist_ok=True)

    log_filename = datetime.now().strftime("log_%Y_%m_%d__%H_%M_%S.log")
    log_path = os.path.join(log_dir, log_filename)

    logging.basicConfig(
        level=LOGS_LEVEL,
        format='[%(asctime)s] %(levelname)s: %(message)s',
        handlers=[logging.FileHandler(log_path, encoding='utf-8'), logging.StreamHandler()]
    )
    return log_path


def find_previous_data_file(current_filename):
    files = [
        f for f in os.listdir(DATA_FOLDER)
        if f.startswith("data_") and f.endswith(".csv") and f != current_filename
    ]

    if not files:
        return None
    def extract_datetime(f):
        try:
            return datetime.strptime(f, "data_%Y_%m_%d__%H_%M_%S.csv")
        except ValueError:
            return datetime.min  # fallback for unexpected files

    files.sort(key=extract_datetime, reverse=True)
    return os.path.join(DATA_FOLDER, files[0])

def generate_record_id(name: str, birth_date: str, counter: int = 0) -> str:
    base = f"{name.strip().lower()}|{birth_date.strip()}"
    if counter > 0:
        base += f"|{counter}"
    return hashlib.md5(base.encode('utf-8')).hexdigest()

def parse_person_text(text: str):
    try:
        logging.debug(f"Raw text: {text}")
        terrorist = '*' in text

        # Extract number at the start
        number_match = re.match(r'(\d+)\.', text)
        number = number_match.group(1) if number_match else "-"

        # Remove number part
        text_no_number = text[number_match.end():].strip() if number_match else text.strip()

        # Extract birth date
        birth_date_match = re.search(r'(\d{2}\.\d{2}\.\d{4}) –≥\.—Ä\.', text_no_number)
        birth_date = birth_date_match.group(1) if birth_date_match else "-"

        # Extract alias in parentheses if present
        alias_match = re.search(r'\(([^()]*)\)', text_no_number)
        second_name_in_braces = clean_field(alias_match.group(1)) if alias_match else "-"

        # Remove alias and birth date for name extraction
        cleaned_text = text_no_number
        if alias_match:
            cleaned_text = cleaned_text.replace(alias_match.group(0), '')
        if birth_date_match:
            cleaned_text = cleaned_text.replace(birth_date_match.group(0), '')

        # Extract name (everything before first comma or double comma)
        parts = [p.strip() for p in cleaned_text.split(',') if p.strip()]
        name = parts[0].replace('*', '') if parts else "-"
        name = clean_field(name)

        # Other data = everything remaining after birth date
        other_data = ""
        if birth_date_match:
            other_data = text_no_number[birth_date_match.end():].strip()
        elif len(parts) > 1:
            other_data = ', '.join(parts[1:]).strip()
        other_data = clean_field(other_data)
        other_data = other_data if other_data else "-"

        birth_date = clean_field(birth_date)

        return [number, name, second_name_in_braces, birth_date, other_data, terrorist, text]

    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏: {e}")
        return ["–û—à–∏–±–∫–∞", "", "", "", "", ""]

def clean_old_files(folder_path, keep_latest=10):
    files = sorted(glob.glob(os.path.join(folder_path, "*")), key=os.path.getmtime, reverse=True)
    for old_file in files[keep_latest:]:
        try:
            os.remove(old_file)
        except Exception as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

def compare_with_previous(old_df, new_df):
    old_ids = set(old_df['ID'])
    new_ids = set(new_df['ID'])

    added_ids = new_ids - old_ids
    removed_ids = old_ids - new_ids
    common_ids = old_ids & new_ids
    changed_ids = set()

    logging.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–µ–π: {len(old_df)}")
    logging.info(f"–ù–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–µ–π: {len(new_df)}")
    logging.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ: {len(added_ids)}")
    logging.info(f"–£–¥–∞–ª–µ–Ω–æ: {len(removed_ids)}")
    logging.info(f"–û–±—â–∏—Ö: {len(common_ids)}")

    # Convert DataFrames to dicts: ID -> row (as dict)
    old_dict = old_df.set_index('ID').to_dict(orient='index')
    new_dict = new_df.set_index('ID').to_dict(orient='index')

    added_list = []
    removed_list = []
    changed_list = []

    # added
    for id_ in added_ids:
        added_list.append(new_dict.get(id_))

    # removed
    for id_ in removed_ids:
        removed_list.append(old_dict.get(id_))

    # changed
    for id_ in common_ids:
        old_row = old_dict.get(id_)
        new_row = new_dict.get(id_)

        # Compare only selected fields
        if (
            old_row.get('–û—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ') != new_row.get('–û—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ') or
            old_row.get('–î–æ–ø –ò–º—è') != new_row.get('–î–æ–ø –ò–º—è') or
            old_row.get('–¢–µ—Ä—Ä–æ—Ä–∏—Å—Ç') != new_row.get('–¢–µ—Ä—Ä–æ—Ä–∏—Å—Ç')
        ):
            changed_ids.add(id_)
            changed_list.append((old_row, new_row))

    logging.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ –æ–±—â–∏—Ö: {len(changed_ids)}")

    return added_list, removed_list, changed_list

    # report = ""
    # if added_ids:
    #     report += f"–î–æ–±–∞–≤–ª–µ–Ω—ã: {len(added_ids)}\n"
    # if removed_ids:
    #     report += f"–£–¥–∞–ª–µ–Ω—ã: {len(removed_ids)}\n"
    # if changed_ids:
    #     report += f"–ò–∑–º–µ–Ω–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(changed_ids)}\n"

    # return report, added_ids, removed_ids, changed_ids

def parse_data():
    log_path = setup_logger()
    logging.info("=== –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –†–§–ú ===")
    logging.info(f"–õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤: {log_path}")

    os.makedirs(DATA_FOLDER, exist_ok=True)
    os.makedirs(LOGS_FOLDER, exist_ok=True)
    os.makedirs(AIRTABLE_FOLDER, exist_ok=True)

    clean_old_files(DATA_FOLDER, KEEP_FILES_COUNT)
    clean_old_files(LOGS_FOLDER, KEEP_FILES_COUNT)
    clean_old_files(AIRTABLE_FOLDER, KEEP_FILES_COUNT)

    data_filename = datetime.now().strftime("data_%Y_%m_%d__%H_%M_%S.csv")
    data_filepath = os.path.join(DATA_FOLDER, data_filename)
    prev_filename = find_previous_data_file(data_filename)

    try:
        send_telegram_message("üöÄ –ù–∞—á–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...")

        response = requests.get(PARSE_URL, headers={"User-Agent": "Mozilla/5.0"}, verify=False)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        section = soup.select_one("#russianFL")

        if not section:
            raise ValueError("–°–µ–∫—Ü–∏—è #russianFL –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ HTML.")

        persons_list = section.find_all("li")
        data = []
        id_counter = {}

        for person in persons_list:
            text = person.get_text(strip=True)
            parsed = parse_person_text(text)
            if parsed[0] == "–û—à–∏–±–∫–∞":
                continue

            number, name, alias, birth_date, other_data, terrorist, raw_text = parsed
            key = (name.lower(), birth_date)
            id_counter.setdefault(key, 0)
            record_id = generate_record_id(name, birth_date, id_counter[key])
            id_counter[key] += 1

            data.append([record_id, number, name, alias, birth_date, other_data, terrorist, raw_text])

        if not data:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")

        df = pd.DataFrame(data, columns=["ID", "–ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä", "–ò–º—è", "–î–æ–ø –ò–º—è", "–î–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è", "–û—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "–¢–µ—Ä—Ä–æ—Ä–∏—Å—Ç", "–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç"])
        df.to_csv(data_filepath, index=False, encoding='utf-8')
        logging.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data_filepath}")

        summary = f"#–æ—Ç—á–µ—Ç"
        summary += f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –ó–∞–ø–∏—Å–µ–π –Ω–∞–π–¥–µ–Ω–æ: {len(data)}"
        summary += f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {data_filepath}"
        report_file = None

        if prev_filename and os.path.exists(prev_filename):
            logging.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã–π —Ñ–∞–π–ª {prev_filename}")

            old_df = pd.read_csv(prev_filename, dtype=str)
            new_df = pd.read_csv(data_filepath, dtype=str)

            added_list, removed_list, changed_list = compare_with_previous(old_df, new_df)

            summary += f"\n\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å: {prev_filename}; –ë—ã–ª–æ –∑–∞–ø–∏—Å–µ–π: {len(old_df)}"
            summary += f"\n- –î–æ–±–∞–≤–ª–µ–Ω–æ: {len(added_list)};{'–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –±—É–¥–µ—Ç!' if len(added_list) >= 100 else ''}"
            summary += f"\n- –£–¥–∞–ª–µ–Ω–æ: {len(removed_list)};{'–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –±—É–¥–µ—Ç!' if len(removed_list) >= 100 else ''}"
            summary += f"\n- –û–±–Ω–æ–≤–ª–µ–Ω–æ: {len(changed_list)};{'–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –±—É–¥–µ—Ç!' if len(changed_list) >= 100 else ''}"

            send_telegram_message(summary)
            send_telegram_file(data_filepath)

            report = []

            index = 0
            for added_row in added_list:
                index += 1
                msg = "#–¥–æ–±–∞–≤–ª–µ–Ω"
                msg += f"\n{added_row.get('–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç') or added_row.get('–ò–º—è')}"

                report.append(msg)

                if index < 100:
                    send_telegram_message(msg)

            index = 0
            for removed_row in removed_list:
                index += 1

                msg = "#—É–¥–∞–ª–µ–Ω"
                msg += f"\n{removed_row.get('–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç') or removed_list.get('–ò–º—è')}"

                report.append(msg)
            
                if index < 100:
                    send_telegram_message(msg)

            index = 0
            for old_row, new_row in changed_list:
                index += 1

                msg = "#–æ–±–Ω–æ–≤–ª–µ–Ω (–±—ã–ª–æ/—Å—Ç–∞–ª–æ)"
                msg += f"\n\n{old_row.get('–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç') or old_row.get('–ò–º—è')}"
                msg += f"\n\n{new_row.get('–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç') or new_row.get('–ò–º—è')}"
    
                report.append(msg)

                if index < 100:
                    send_telegram_message(msg)

            if report:
                report_filename = os.path.splitext(data_filename)[0] + "_report.txt"
                report_file = os.path.join(DATA_FOLDER, report_filename)
                with open(report_file, "w", encoding="utf-8") as f:
                    report = "\n----------\n".join(report)
                    f.write(report)

                if report_file:
                    send_telegram_file(report_file)

        else:
            logging.info("–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫. –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.")
            summary += "\n\nüìÇ –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫. –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."

            send_telegram_message(summary)
            send_telegram_file(data_filepath)


    except Exception as e:
        logging.error("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞:", exc_info=True)
        send_telegram_message(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")

    logging.info("=== –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã ===")
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä —Ä–µ–µ—Å—Ç—Ä–∞ –†–§–ú (fedsfm.ru)")
    args = parser.parse_args()

    parse_data()
